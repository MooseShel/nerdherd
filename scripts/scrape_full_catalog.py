
import requests
from bs4 import BeautifulSoup
import json
import time
import re
import sys

# Requirements: pip install requests beautifulsoup4

class CourseScraper:
    def __init__(self):
        self.courses = []
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def clean_text(self, text):
        if not text: return ""
        return " ".join(text.split())

    def save_sql(self, filename="database/import_all_majors.sql"):
        print(f"Saving {len(self.courses)} courses to {filename}...")
        
        # Split into chunks to avoid huge transactions if needed, but one file is usually fine for < 10k rows
        # We will use ON CONFLICT DO UPDATE to ensure we don't break existing
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("-- Full Catalog Import for All Majors\n")
            f.write("-- Generated by scrape_full_catalog.py\n\n")
            
            f.write("DO $$\n")
            f.write("DECLARE\n")
            f.write("  uh_id UUID;\n")
            f.write("  hcc_id UUID;\n")
            f.write("  dept_id UUID;\n")
            f.write("BEGIN\n")
            f.write("  SELECT id INTO uh_id FROM universities WHERE short_name = 'UH';\n")
            f.write("  SELECT id INTO hcc_id FROM universities WHERE short_name = 'HCCS';\n\n")
            
            # Group by University -> Department
            courses_by_uni = {}
            for c in self.courses:
                u = c['university']
                if u not in courses_by_uni: courses_by_uni[u] = {}
                
                dept = c['department_code']
                if dept not in courses_by_uni[u]: courses_by_uni[u][dept] = []
                
                courses_by_uni[u][dept].append(c)
            
            for uni, depts in courses_by_uni.items():
                uni_var = 'uh_id' if uni == 'UH' else 'hcc_id'
                
                for dept_code, dept_courses in depts.items():
                    # clean dept name
                    dept_name = dept_courses[0]['department_name'].replace("'", "''")
                    
                    f.write(f"  -- {uni} : {dept_code}\n")
                    f.write(f"  INSERT INTO departments (university_id, name, code) VALUES ({uni_var}, '{dept_name}', '{dept_code}') ON CONFLICT (university_id, code) DO NOTHING;\n")
                    f.write(f"  SELECT id INTO dept_id FROM departments WHERE university_id = {uni_var} AND code = '{dept_code}';\n")
                    
                    f.write("  INSERT INTO courses (university_id, department_id, course_code, course_number, title, credits, description) VALUES\n")
                    
                    values = []
                    for c in dept_courses:
                        c_code = c['course_code'].replace("'", "''")
                        c_num = c['course_number']
                        title = c['title'].replace("'", "''")
                        desc = c['description'].replace("'", "''")
                        credits = c['credits']
                        
                        values.append(f"    ({uni_var}, dept_id, '{c_code}', '{c_num}', '{title}', {credits}, '{desc}')")
                    
                    f.write(",\n".join(values))
                    f.write("\n  ON CONFLICT (university_id, course_code) DO UPDATE SET title = EXCLUDED.title, credits = EXCLUDED.credits, description = EXCLUDED.description;\n\n")
            
            f.write("END $$;\n")

    # ==========================================
    # UH SCRAPER (Acalog System)
    # ==========================================
    def scrape_uh(self):
        print("Starting UH Scrape...")
        base_url = "https://publications.uh.edu"
        # 2025-2026 Catalog usually has a specific catoid. 
        # We will try to find the "Undergraduate Catalog 2025-2026" link from the main page
        
        try:
            r = requests.get(base_url, headers=self.headers)
            soup = BeautifulSoup(r.text, 'html.parser')
            
            # Find catalog link. This is tricky as ID changes.
            # Look for active catalog in dropdown or list
            # For now, let's assume a search for the text "2025-2026 Undergraduate Catalog"
            cat_link = None
            catalogs = soup.select('a[href*="catalog.php"]')
            for cat in catalogs:
                if "2025-2026" in cat.text and "Undergraduate" in cat.text:
                    cat_link = cat['href']
                    break
            
            # Fallback to hardcoded known recent ID if detection fails (often catoid 52 or 49)
            # URL: content.php?catoid=X&navoid=Y
            # We need the "Course Descriptions" navoid.
            
            # Let's try to hit the search page directly if we can matches?
            # Actually, browsing by prefix is easier.
            
            # Hardcoded strategy for Acalog:
            # 1. Get the 'catoid' (Catalog ID).
            # 2. Use the AJAX filter endpoint: /ajax/preview_filter.php
            # Arguments: catoid=X, cpage=1, location=3 (Courses), filter[27]=-1 (All prefixes)
            
            # Let's find the current catoid.
            catoid = "52" # Speculative for 2025, but let's try to extract from a redirected URL
            
            # Try specific recent catalog URL
            catalog_url = "https://publications.uh.edu/content.php?catoid=52&navoid=19036" # Example for 2024-2025
            r = requests.get("https://publications.uh.edu/preview/2025-2026/undergraduate-catalog", allow_redirects=True)
            if "catoid=" in r.url:
                catoid = re.search(r'catoid=(\d+)', r.url).group(1)
                print(f"Detected UH Catoid: {catoid}")
            
            # Now find the "Courses" link (navoid)
            # Usually text "Course Descriptions" text in the sidebar
            navoid = None
            soup = BeautifulSoup(r.text, 'html.parser')
            links = soup.find_all('a')
            for l in links:
                if "Course Descriptions" in l.text:
                   href = l.get('href', '')
                   if "navoid=" in href:
                       navoid = re.search(r'navoid=(\d+)', href).group(1)
                       break
            
            if not navoid:
                print("Could not find Course Descriptions link. Using fallback or full text search.")
                # We can fallback to crawling the search page A-Z
                # But better to ask user to check
                return

            print(f"Detected UH Navoid: {navoid}")
            
            # Fetch the pagination pages
            # Acalog pages: content.php?catoid=X&navoid=Y&cpage=Z
            cpage = 1
            while True:
                time.sleep(1) # Be nice
                page_url = f"{base_url}/content.php?catoid={catoid}&navoid={navoid}&cpage={cpage}"
                print(f"Scraping UH Page {cpage}...")
                
                r = requests.get(page_url, headers=self.headers)
                soup = BeautifulSoup(r.text, 'html.parser')
                
                # Acalog usually puts courses in a table or list
                # Look for links that open preview: class="popup-with-zoom-anim" or onclick="showCourse"
                # Actually, main list usually has links like: preview_course_nopop.php?catoid=X&coid=Y
                
                course_links = soup.select("a[href*='preview_course']")
                if not course_links:
                    print("No courses found on this page. Reaching end?")
                    break
                
                found_new = False
                for link in course_links:
                    text = self.clean_text(link.text)
                    # Format: "ACCT 2301 - Principles of Financial Accounting"
                    # Regex: ^([A-Z]{3,4})\s(\d{4})\s-\s(.*)$
                    match = re.match(r'^([A-Z]{3,4})\s(\d{4})\s-\s(.*)$', text)
                    if match:
                        dept_code = match.group(1)
                        course_num = match.group(2)
                        title = match.group(3)
                        
                        # Get details? We need credits. 
                        # Sometimes credits are in the text after the link or we need to click.
                        # For speed, let's default to 3 if not found, or try to scrape from row text.
                        # The link is usually inside a <td> or <li>. Let's look at parent text.
                        row_text = self.clean_text(link.parent.text)
                        
                        credits = 3
                        # "Credit Hours: 3.0"
                        cred_match = re.search(r'Credit Hours:\s*([\d\.]+)', row_text)
                        if cred_match:
                            try:
                                credits = int(float(cred_match.group(1)))
                            except:
                                pass
                        
                        # Description? Hidden in the popup. 
                        # Crawling every popup for 3000 courses is slow.
                        # Let's generate a placeholder description from title if speed is key.
                        # "Principles of Financial Accounting course at UH"
                        desc = f"{title} - {dept_code} course at University of Houston"
                        
                        # Dept Name mapping? 
                        # We can guess from code or just use the Code as Name for now if needed.
                        # Or manual map.
                        dept_name = dept_code # Placeholder
                        
                        self.courses.append({
                            'university': 'UH',
                            'department_code': dept_code,
                            'department_name': dept_code, # Improving this would require a map
                            'course_code': f"{dept_code} {course_num}",
                            'course_number': course_num,
                            'title': title,
                            'credits': credits,
                            'description': desc
                        })
                        found_new = True
                
                if not found_new and cpage > 1:
                    break
                
                # Check for "Next" link? 
                # Acalog pages often just list 1,2,3... or [Next]
                next_link = soup.find('a', text=re.compile(r'Next'))
                if not next_link:
                    # Try to see if current page is last in list
                    # Maybe just try up to 20 pages max to be safe
                    if cpage > 50: break
                
                cpage += 1
                
        except Exception as e:
            print(f"Error scraping UH: {e}")

    # ==========================================
    # HCCS SCRAPER
    # ==========================================
    def scrape_hccs(self):
        print("Starting HCCS Scrape...")
        # HCCS is simpler or harder? 
        # https://catalog.hccs.edu/ usually
        base_url = "https://catalog.hccs.edu"
        
        # Similar Acalog structure often
        try:
           # Find current catalog
           r = requests.get(base_url, headers=self.headers)
           # Follow redirect
           
           # Need to find "Course Descriptions"
           # ... implementation similar to UH ...
           # For brevity in this snippet, let's implement a dummy verification
           # that prompts user to check URL if automatic fails.
           pass 
           
           # NOTE: Since HCCS often uses Acalog too, the logic is identical.
           # We would just need the HCCS catoid/navoid.
           
        except Exception as e:
            print(f"Error scraping HCCS: {e}")

if __name__ == "__main__":
    scraper = CourseScraper()
    scraper.scrape_uh()
    # scraper.scrape_hccs() # Enable if verified
    
    # Save
    if scraper.courses:
        scraper.save_sql()
        print("Done! Run 'database/import_all_majors.sql' in Supabase.")
    else:
        print("No courses found. Please check network/URLs.")
